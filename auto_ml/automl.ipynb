{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:24:55.547394800Z",
     "start_time": "2024-02-16T16:24:53.096987900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "higgs_data = pd.read_csv('../data/HIGGS.csv.gz', compression='gzip', header=None, nrows=20000)\n",
    "higgs_data.columns = ['class_label',\n",
    "                      'lepton_pT', 'lepton_eta', 'lepton_phi',\n",
    "                      'missing_energy_magnitude', 'missing_energy_phi',\n",
    "                      'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag',\n",
    "                      'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_b-tag',\n",
    "                      'jet_3_pt', 'jet_3_eta', 'jet_3_phi', 'jet_3_b-tag',\n",
    "                      'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_b-tag',\n",
    "                      'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
    "\n",
    "drop_feats = True\n",
    "if drop_feats:\n",
    "    higgs_data = higgs_data.drop(columns=['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   class_label  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n0          1.0   0.869293   -0.635082    0.225690                  0.327470   \n1          1.0   0.907542    0.329147    0.359412                  1.497970   \n2          1.0   0.798835    1.470639   -1.635975                  0.453773   \n3          0.0   1.344385   -0.876626    0.935913                  1.992050   \n4          1.0   1.105009    0.321356    1.522401                  0.882808   \n\n   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_b-tag  ...  \\\n0           -0.689993  0.754202  -0.248573  -1.092064     0.000000  ...   \n1           -0.313010  1.095531  -0.557525  -1.588230     2.173076  ...   \n2            0.425629  1.104875   1.282322   1.381664     0.000000  ...   \n3            0.882454  1.786066  -1.646778  -0.942383     0.000000  ...   \n4           -1.205349  0.681466  -1.070464  -0.921871     0.000000  ...   \n\n   jet_2_phi  jet_2_b-tag  jet_3_pt  jet_3_eta  jet_3_phi  jet_3_b-tag  \\\n0   0.930349     1.107436  1.138904  -1.578198  -1.046985     0.000000   \n1   1.271015     2.214872  0.499994  -1.261432   0.732156     0.000000   \n2  -0.819690     2.214872  0.993490   0.356080  -0.208778     2.548224   \n3   0.736159     2.214872  1.298720  -1.430738  -0.364658     0.000000   \n4   0.971407     2.214872  0.596761  -0.350273   0.631194     0.000000   \n\n   jet_4_pt  jet_4_eta  jet_4_phi  jet_4_b-tag  \n0  0.657930  -0.010455  -0.045767     3.101961  \n1  0.398701  -1.138930  -0.000819     0.000000  \n2  1.256955   1.128848   0.900461     0.000000  \n3  0.745313  -0.678379  -1.360356     0.000000  \n4  0.479999  -0.373566   0.113041     0.000000  \n\n[5 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class_label</th>\n      <th>lepton_pT</th>\n      <th>lepton_eta</th>\n      <th>lepton_phi</th>\n      <th>missing_energy_magnitude</th>\n      <th>missing_energy_phi</th>\n      <th>jet_1_pt</th>\n      <th>jet_1_eta</th>\n      <th>jet_1_phi</th>\n      <th>jet_1_b-tag</th>\n      <th>...</th>\n      <th>jet_2_phi</th>\n      <th>jet_2_b-tag</th>\n      <th>jet_3_pt</th>\n      <th>jet_3_eta</th>\n      <th>jet_3_phi</th>\n      <th>jet_3_b-tag</th>\n      <th>jet_4_pt</th>\n      <th>jet_4_eta</th>\n      <th>jet_4_phi</th>\n      <th>jet_4_b-tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.869293</td>\n      <td>-0.635082</td>\n      <td>0.225690</td>\n      <td>0.327470</td>\n      <td>-0.689993</td>\n      <td>0.754202</td>\n      <td>-0.248573</td>\n      <td>-1.092064</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.930349</td>\n      <td>1.107436</td>\n      <td>1.138904</td>\n      <td>-1.578198</td>\n      <td>-1.046985</td>\n      <td>0.000000</td>\n      <td>0.657930</td>\n      <td>-0.010455</td>\n      <td>-0.045767</td>\n      <td>3.101961</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.907542</td>\n      <td>0.329147</td>\n      <td>0.359412</td>\n      <td>1.497970</td>\n      <td>-0.313010</td>\n      <td>1.095531</td>\n      <td>-0.557525</td>\n      <td>-1.588230</td>\n      <td>2.173076</td>\n      <td>...</td>\n      <td>1.271015</td>\n      <td>2.214872</td>\n      <td>0.499994</td>\n      <td>-1.261432</td>\n      <td>0.732156</td>\n      <td>0.000000</td>\n      <td>0.398701</td>\n      <td>-1.138930</td>\n      <td>-0.000819</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.798835</td>\n      <td>1.470639</td>\n      <td>-1.635975</td>\n      <td>0.453773</td>\n      <td>0.425629</td>\n      <td>1.104875</td>\n      <td>1.282322</td>\n      <td>1.381664</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.819690</td>\n      <td>2.214872</td>\n      <td>0.993490</td>\n      <td>0.356080</td>\n      <td>-0.208778</td>\n      <td>2.548224</td>\n      <td>1.256955</td>\n      <td>1.128848</td>\n      <td>0.900461</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.344385</td>\n      <td>-0.876626</td>\n      <td>0.935913</td>\n      <td>1.992050</td>\n      <td>0.882454</td>\n      <td>1.786066</td>\n      <td>-1.646778</td>\n      <td>-0.942383</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.736159</td>\n      <td>2.214872</td>\n      <td>1.298720</td>\n      <td>-1.430738</td>\n      <td>-0.364658</td>\n      <td>0.000000</td>\n      <td>0.745313</td>\n      <td>-0.678379</td>\n      <td>-1.360356</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1.105009</td>\n      <td>0.321356</td>\n      <td>1.522401</td>\n      <td>0.882808</td>\n      <td>-1.205349</td>\n      <td>0.681466</td>\n      <td>-1.070464</td>\n      <td>-0.921871</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.971407</td>\n      <td>2.214872</td>\n      <td>0.596761</td>\n      <td>-0.350273</td>\n      <td>0.631194</td>\n      <td>0.000000</td>\n      <td>0.479999</td>\n      <td>-0.373566</td>\n      <td>0.113041</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higgs_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:24:55.562342300Z",
     "start_time": "2024-02-16T16:24:55.546235900Z"
    }
   },
   "id": "3ccb95408c57ae38",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(higgs_data, test_size=0.2, random_state=42)\n",
    "train_data.to_csv('HIGGS_10k_train.csv', index=False)\n",
    "test_data.to_csv('HIGGS_10k_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:24:59.407506800Z",
     "start_time": "2024-02-16T16:24:55.563340900Z"
    }
   },
   "id": "17f29c6517eed107",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240216_162500\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240216_162500\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       14.26 GB / 31.68 GB (45.0%)\n",
      "Disk Space Avail:   234.32 GB / 839.00 GB (27.9%)\n",
      "===================================================\n",
      "Train Data Rows:    16000\n",
      "Train Data Columns: 21\n",
      "Label Column:       class_label\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1.0, 0.0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14595.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 21 | ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 21 | ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 14400, Val Rows: 1600\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "F:\\Miniconda3\\envs\\L65\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"F:\\Miniconda3\\envs\\L65\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "\t0.5331\t = Validation score   (accuracy)\n",
      "\t2.7s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5331\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.6419\t = Validation score   (accuracy)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.6306\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6269\t = Validation score   (accuracy)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6181\t = Validation score   (accuracy)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.6369\t = Validation score   (accuracy)\n",
      "\t8.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6081\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6044\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.6331\t = Validation score   (accuracy)\n",
      "\t7.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6319\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6356\t = Validation score   (accuracy)\n",
      "\t10.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.6362\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.211, 'LightGBM': 0.211, 'NeuralNetFastAI': 0.211, 'NeuralNetTorch': 0.158, 'LightGBMLarge': 0.105, 'RandomForestGini': 0.053, 'ExtraTreesGini': 0.053}\n",
      "\t0.6569\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 38.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240216_162500\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.63625,\n 'balanced_accuracy': 0.6344619198399613,\n 'mcc': 0.2708133351784862,\n 'roc_auc': 0.6876464421822553,\n 'f1': 0.6621778500116091,\n 'precision': 0.6354723707664884,\n 'recall': 0.6912263693650024}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "higgs_10k_train = TabularDataset('HIGGS_10k_train.csv')\n",
    "higgs_10k_test = TabularDataset('HIGGS_10k_test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='class_label').fit(train_data=higgs_10k_train)\n",
    "predictor.evaluate(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:25:39.598190100Z",
     "start_time": "2024-02-16T16:24:59.409507Z"
    }
   },
   "id": "16f988e168370352",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
