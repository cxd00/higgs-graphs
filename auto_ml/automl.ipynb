{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:51:07.792898Z",
     "start_time": "2024-03-14T00:51:05.449855Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "higgs_data = pd.read_csv('../data/HIGGS.csv.gz', compression='gzip', header=None, nrows=20000)\n",
    "higgs_data.columns = ['class_label',\n",
    "                      'lepton_pT', 'lepton_eta', 'lepton_phi',\n",
    "                      'missing_energy_magnitude', 'missing_energy_phi',\n",
    "                      'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag',\n",
    "                      'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_b-tag',\n",
    "                      'jet_3_pt', 'jet_3_eta', 'jet_3_phi', 'jet_3_b-tag',\n",
    "                      'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_b-tag',\n",
    "                      'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
    "\n",
    "drop_feats = False\n",
    "if drop_feats:\n",
    "    higgs_data = higgs_data.drop(columns=['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   class_label  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n0          1.0   0.869293   -0.635082    0.225690                  0.327470   \n1          1.0   0.907542    0.329147    0.359412                  1.497970   \n2          1.0   0.798835    1.470639   -1.635975                  0.453773   \n3          0.0   1.344385   -0.876626    0.935913                  1.992050   \n4          1.0   1.105009    0.321356    1.522401                  0.882808   \n\n   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_b-tag  ...  \\\n0           -0.689993  0.754202  -0.248573  -1.092064     0.000000  ...   \n1           -0.313010  1.095531  -0.557525  -1.588230     2.173076  ...   \n2            0.425629  1.104875   1.282322   1.381664     0.000000  ...   \n3            0.882454  1.786066  -1.646778  -0.942383     0.000000  ...   \n4           -1.205349  0.681466  -1.070464  -0.921871     0.000000  ...   \n\n   jet_4_eta  jet_4_phi  jet_4_b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n0  -0.010455  -0.045767     3.101961  1.353760  0.979563  0.978076  0.920005   \n1  -1.138930  -0.000819     0.000000  0.302220  0.833048  0.985700  0.978098   \n2   1.128848   0.900461     0.000000  0.909753  1.108330  0.985692  0.951331   \n3  -0.678379  -1.360356     0.000000  0.946652  1.028704  0.998656  0.728281   \n4  -0.373566   0.113041     0.000000  0.755856  1.361057  0.986610  0.838085   \n\n       m_bb     m_wbb    m_wwbb  \n0  0.721657  0.988751  0.876678  \n1  0.779732  0.992356  0.798343  \n2  0.803252  0.865924  0.780118  \n3  0.869200  1.026736  0.957904  \n4  1.133295  0.872245  0.808487  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class_label</th>\n      <th>lepton_pT</th>\n      <th>lepton_eta</th>\n      <th>lepton_phi</th>\n      <th>missing_energy_magnitude</th>\n      <th>missing_energy_phi</th>\n      <th>jet_1_pt</th>\n      <th>jet_1_eta</th>\n      <th>jet_1_phi</th>\n      <th>jet_1_b-tag</th>\n      <th>...</th>\n      <th>jet_4_eta</th>\n      <th>jet_4_phi</th>\n      <th>jet_4_b-tag</th>\n      <th>m_jj</th>\n      <th>m_jjj</th>\n      <th>m_lv</th>\n      <th>m_jlv</th>\n      <th>m_bb</th>\n      <th>m_wbb</th>\n      <th>m_wwbb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.869293</td>\n      <td>-0.635082</td>\n      <td>0.225690</td>\n      <td>0.327470</td>\n      <td>-0.689993</td>\n      <td>0.754202</td>\n      <td>-0.248573</td>\n      <td>-1.092064</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.010455</td>\n      <td>-0.045767</td>\n      <td>3.101961</td>\n      <td>1.353760</td>\n      <td>0.979563</td>\n      <td>0.978076</td>\n      <td>0.920005</td>\n      <td>0.721657</td>\n      <td>0.988751</td>\n      <td>0.876678</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.907542</td>\n      <td>0.329147</td>\n      <td>0.359412</td>\n      <td>1.497970</td>\n      <td>-0.313010</td>\n      <td>1.095531</td>\n      <td>-0.557525</td>\n      <td>-1.588230</td>\n      <td>2.173076</td>\n      <td>...</td>\n      <td>-1.138930</td>\n      <td>-0.000819</td>\n      <td>0.000000</td>\n      <td>0.302220</td>\n      <td>0.833048</td>\n      <td>0.985700</td>\n      <td>0.978098</td>\n      <td>0.779732</td>\n      <td>0.992356</td>\n      <td>0.798343</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.798835</td>\n      <td>1.470639</td>\n      <td>-1.635975</td>\n      <td>0.453773</td>\n      <td>0.425629</td>\n      <td>1.104875</td>\n      <td>1.282322</td>\n      <td>1.381664</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.128848</td>\n      <td>0.900461</td>\n      <td>0.000000</td>\n      <td>0.909753</td>\n      <td>1.108330</td>\n      <td>0.985692</td>\n      <td>0.951331</td>\n      <td>0.803252</td>\n      <td>0.865924</td>\n      <td>0.780118</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.344385</td>\n      <td>-0.876626</td>\n      <td>0.935913</td>\n      <td>1.992050</td>\n      <td>0.882454</td>\n      <td>1.786066</td>\n      <td>-1.646778</td>\n      <td>-0.942383</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.678379</td>\n      <td>-1.360356</td>\n      <td>0.000000</td>\n      <td>0.946652</td>\n      <td>1.028704</td>\n      <td>0.998656</td>\n      <td>0.728281</td>\n      <td>0.869200</td>\n      <td>1.026736</td>\n      <td>0.957904</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1.105009</td>\n      <td>0.321356</td>\n      <td>1.522401</td>\n      <td>0.882808</td>\n      <td>-1.205349</td>\n      <td>0.681466</td>\n      <td>-1.070464</td>\n      <td>-0.921871</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.373566</td>\n      <td>0.113041</td>\n      <td>0.000000</td>\n      <td>0.755856</td>\n      <td>1.361057</td>\n      <td>0.986610</td>\n      <td>0.838085</td>\n      <td>1.133295</td>\n      <td>0.872245</td>\n      <td>0.808487</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 29 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higgs_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:51:07.823150Z",
     "start_time": "2024-03-14T00:51:07.793903Z"
    }
   },
   "id": "3ccb95408c57ae38",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(higgs_data, test_size=0.2, random_state=42)\n",
    "train_data.to_csv('HIGGS_10k_train.csv', index=False)\n",
    "test_data.to_csv('HIGGS_10k_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:51:10.993107Z",
     "start_time": "2024-03-14T00:51:07.824149Z"
    }
   },
   "id": "17f29c6517eed107",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240314_005111\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240314_005111\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       15.87 GB / 31.68 GB (50.1%)\n",
      "Disk Space Avail:   203.97 GB / 839.00 GB (24.3%)\n",
      "===================================================\n",
      "Train Data Rows:    16000\n",
      "Train Data Columns: 28\n",
      "Label Column:       class_label\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1.0, 0.0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16248.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.42 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 28 | ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 28 | ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t28 features in original data used to generate 28 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.42 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 14400, Val Rows: 1600\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "F:\\Miniconda3\\envs\\L65\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"F:\\Miniconda3\\envs\\L65\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "\t0.5494\t = Validation score   (accuracy)\n",
      "\t3.65s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5494\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7188\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7194\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7212\t = Validation score   (accuracy)\n",
      "\t2.92s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7106\t = Validation score   (accuracy)\n",
      "\t3.28s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7181\t = Validation score   (accuracy)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6819\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.695\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7075\t = Validation score   (accuracy)\n",
      "\t8.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7163\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7019\t = Validation score   (accuracy)\n",
      "\t7.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7238\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.327, 'LightGBMLarge': 0.255, 'LightGBM': 0.109, 'XGBoost': 0.091, 'ExtraTreesGini': 0.073, 'RandomForestGini': 0.055, 'CatBoost': 0.055, 'ExtraTreesEntr': 0.036}\n",
      "\t0.735\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 35.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240314_005111\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.705,\n 'balanced_accuracy': 0.7041709636386705,\n 'mcc': 0.40901762537666553,\n 'roc_auc': 0.7859919004632346,\n 'f1': 0.7186456843109204,\n 'precision': 0.7071797278273111,\n 'recall': 0.7304895782840524}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "higgs_10k_train = TabularDataset('HIGGS_10k_train.csv')\n",
    "higgs_10k_test = TabularDataset('HIGGS_10k_test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='class_label').fit(train_data=higgs_10k_train)\n",
    "predictor.evaluate(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:51:48.138848Z",
     "start_time": "2024-03-14T00:51:10.994108Z"
    }
   },
   "id": "16f988e168370352",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
